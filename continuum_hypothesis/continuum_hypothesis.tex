\documentclass[letterpaper,12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{url}

\setlength{\voffset}{-1 in}
\addtolength{\textheight}{2.15 in}

\renewcommand{\implies}{\rightarrow}
\newcommand{\pr}{\text{pr}}
\newcommand{\Prob}{\text{Pr}}
%\newcommand{\PR}{\text{PR}}
\newcommand{\dom}{\mathrm{dom\ }}
\newcommand{\rng}{\mathrm{rng\ }}

\newcommand{\A}{\mathfrak{A}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newcommand{\ZFC}{\mathrm{ZFC}}


\renewcommand{\O}{\mathfrak{O}}
\newcommand{\T}{\mathfrak{T}}
\newcommand{\proves}{\vdash}

\renewcommand{\phi}{\varphi}

\newcommand{\hajek}{H\'ajek}

\newtheorem{criterion}{Criterion}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{claim}{Claim}


\begin{document}
\bibliographystyle{plainnat}

\title{Can we resolve the Continuum Hypothesis?}
\author{Shivaram Lingamneni}
\maketitle


\begin{abstract}
I argue that that contemporary set theory, as depicted in the 2011-2012 EFI lecture series, lacks a program that promises to decide, in a genuinely platonistic fashion, the continuum hypothesis (CH) and related questions about the ``width'' of the universe. We can distinguish three possible objectives for a platonistic completion of set theory: maximizing structures, maximizing sets, and maximizing interpretive power. However, none of these is allied to a program that can plausibly decide CH. I discuss the implications of this for set theory and other foundational programs.
\end{abstract}

\section{Introduction}
The continuum hypothesis (CH) --- the hypothesis or conjecture that $2^{\aleph_0} = \aleph_1$ --- is as old as set theory itself and has cast its long shadow over the discipline for the entirety of their shared history. As early as 1878, Cantor asked the question in its modern form: is every $X \subseteq \R$ in bijection with either $\N$ or $\R$? By 1900, the question was well established enough to be the first of the 23 Hilbert problems, unsolved questions that would guide the future of mathematical research for much of the 20th century. And its inclusion in the list did bear immediate fruit in shaping the evolution of descriptive set theory; for example, interest in the perfect set property was inspired by Cantor's search for subsets of $\mathbb{R}$ that could be counterexamples to CH \citep{kanamori2008higher}.

Real progress on the original question, however, had to wait for G\"odel's 1938 identification of the constructible universe $L$, an inner model of any model of set theory which always satisfies the continuum hypothesis; this showed the equiconsistency of ZFC with ZFC + CH. Then, in another conceptual breakthrough, Cohen's work of 1963 showed, via the novel technique of forcing, that ZFC is also equiconsistent with ZFC $+ \neg \mathrm{CH}$; this completed the proof that CH is formally independent of ZFC. This proof inaugurated the contemporary era of set theory, characterized by the study of problems independent of ZFC.

In 2011 and 2012, Peter Koellner convened the EFI (``Exploring the Foundations of Incompleteness'') lecture series at Harvard, inviting leading researchers in set theory and related fields to present papers on the philosophical significance of the past half-decade of set-theoretic advances. A variety of perspectives were represented, but the ones I will discuss here formed a sort of spectrum between anti-realism and realism about set theory. At one end of the spectrum, \cite{feferman2011ch} described a position of anti-realism about much of transfinite mathematics, including parts of ZFC itself and extending upwards to CH. In the middle, \cite{hamkins2011set} defended \emph{multiversism} about set theory, i.e., the claim that the independence results have resolved CH definitely by showing that its truth value can vary across a multiverse of equally acceptable models of ZFC, none of which has a privileged claim to being the true $V$. Without taking a strong ontological stance, \cite{cummings2012challenges} argued for a naturalistic acceptance of the independence phenomena as the subject matter of combinatorial set theory. Finally, Magidor, Martin, Steel, and Woodin defended various forms of set-theoretic realism, on which the continuum hypothesis could have a definite truth value that can be discovered through modern set-theoretic research.

My goal here is to argue, with reference to these EFI papers, that no development in contemporary set theory contributes to a \emph{Platonistic} resolution of the continuum hypothesis --- in other words, that all programs that purport to resolve CH are either philosophically unsuccessful, or are implicitly anti-realist about the truth value of CH. In particular, I distinguish three possible goals for a Platonistic completion of set theory: maximizing structures, maximizing sets, and maximizing interpretive power. I will argue that the first goal is revealed, in the light of the independence phenomena, as incoherent; that the second is coherent but not genuinely realized by any contemporary program; and that the third fails to be Platonistic.

I wish to stress that none of this should be taken as maligning the significance of contemporary set theory. First of all, I think the case is clear that independently of the philosophical programs it is in dialogue with, set theory is in its own right a deep and important branch of mathematics. (I refer the skeptical reader to \cite{cummings2012challenges} in particular.) But I hope that my discussion will also make clear my belief that the technical progress in contemporary set theory does have a great deal of philosophical significance.

\subsection{A note on Platonism}
My insistence on a Platonistic resolution of the continuum problem raises the question of what I mean by Platonism. Following G\"odel, questions about the philosophical motivations for new set-theoretic axioms are often thought of in terms of a dichotomy between \emph{intrinsic} justification (i.e., justification on the basis of the philosophical concept of set) and \emph{extrinsic} justification (justification on the basis of some other consideration, such as mathematical fruitfulness); \cite{koellner2009reflection} presents an overview of this distinction. This is not exactly what I mean. In fact, I would prefer to avoid making this discussion rest on a full characterization of Platonism; it should suffice for me to survey the various mathematical-philosophical approaches to resolving CH and then argue that they are in some way essentially \emph{anti-Platonistic}, that is, to rely on negative rather than positive characterizations of Platonism.

Nonetheless, it's appropriate to say something, by means of examples, about the kind of Platonism I have in mind. What \cite{Shapiro2000-SHATAM} calls \emph{realism in ontology} about the set-theoretic universe $V$ --- belief in a mind-independent canonical universe of set theory that fixes the truth values of sentences in the language of set theory --- certainly qualifies as a Platonism. More generally, one could have what Shapiro calls \emph{realism in truth-value} about the language of set theory, or about some fragment of it, without being committed to belief in a unique (or in any) mind-independent $V$. But in functional terms, I am talking about attitudes to set theory on which the value of the continuum is something to be \emph{discovered}, in the sense that working mathematicians in other fields attempt to discover the outcomes of their conjectures, rather than something to be \emph{adjudicated} by professional consensus. In this sense, the hyperuniverse program \citep{arrigoni2013hyperuniverse}, despite its explicit claims not to rest on Platonism in the sense of realism-in-ontology, could potentially lead on my conception to a Platonistic resolution of CH.

\section{Basic independendence phenomena}
\subsection{``Width'' independence results: inner and outer models}
\label{width}
My discussion will focus on the following basic independence phenomena. Given any model of set theory $V$, one can identify within it the inner model $L$, the universe containing only the constructible sets. $L$ is the ``smallest possible'' universe in a precise sense; in particular, it is the minimal class model of ZFC inside $V$ and it is absolute under its own construction, so it is a model of $V=L$ (an axiom saying that every set is constructible). The continuum hypothesis is always true in $L$, regardless of its status in $V$. In fact, so is the \emph{generalized} continuum hypothesis (GCH), which fixes the exponentiation function for the entire cardinal hierarchy at $2^{\aleph_\alpha} = \aleph_{\alpha + 1}$. Moreover, Jensen gave a ``fine-structural'' analysis $L$ that establishes many of its combinatorial properties. In particular, $L$ satisfies the combinatorial principle $\diamondsuit$, which implies the existence of a Suslin line: a dense linear order without endpoints that, like $\mathbb{R}$, is complete and has the countable chain condition, but which is not isomorphic to $\mathbb{R}$. Similarly, Shelah established that if $V = L$, every Whitehead group (an abelian group satisfying $\mathrm{Ext}^1(A, \Z) = 0$) is free.

Meanwhile, forcing is a technique for ``expanding'' a model $V$ (there are metamathematical subtleties here, to which I will return in section \ref{philosophicalsignificance}). A few forcing constructions in particular are of interest to us here: forcing can increase the size of the continuum, creating a model that violates CH from a model in which it holds. It can also decrease the value of the continuum, or ``collapse'' a cardinal (adding a bijection between it and a lesser ordinal, so that it ceases to be a cardinal); for example, one can make $\aleph_1$ into a countable ordinal by forcing. Forcing can also alter the combinatorial properties of $V$. In particular, one can force the negation of CH together with the principle Martin's Axiom (MA).\footnote{I will use MA as a synonym for $\mathrm{MA}(\omega_1)$.} This principle implies that there is no Suslin line, but it also implies the existence of a non-free Whitehead group.

These are the so-called ``width'' independence phenomena, so-called because they do not change the ordinals $\alpha$ of the universe, but do alter the levels $V_\alpha$ of the cumulative hierarchy. Within this metaphor, $L$ is a ``thin'' universe; the levels $L_\alpha$ of its cumulative hierarchy are the smallest possible under ZFC. It has ``few'' reals (indeed, the smallest possible number $\aleph_1$), and its ``orderliness'' manifests itself in its satisfaction of strong combinatorial principles such as $\diamondsuit$. Meanwhile, a universe satisfying MA is metaphorically ``thick'', since the presence of certain objects (generic filters) has been guaranteed.


\subsection{``Height'' independence results: large cardinals}
\label{height}
Essentially, the large cardinals are cardinals such that their existence implies the consistency of ZFC, and therefore (by G\"odel's second incompleteness theorem) cannot be proven within ZFC itself. They can be divided roughly into two groups. The ``small'' large cardinals are those consistent with $V = L$; this category begins with simple properties such as inaccessibility\footnote{A weakly inaccessible $\kappa$ is both regular and limit, so it can't be ``reached'' from below by taking limits or successor cardinals respectively. A strongly inaccessible $\kappa$ additionally can't be reached from below by taking power sets; this implies that $V_\kappa$ is a model of ZFC.} and proceeds through various properties of interest to combinatorial set theorists. Beginning approximately with $0^\#$ and the measurable cardinals, we get cardinals such that their existence is inconsistent with $V = L$; these are the ``large'' large cardinals. It should be noted that it is misleading to view the ``height'' of large cardinals in terms of their literal ordinal height. For example, suppose $V$ contains a measurable cardinal $\kappa$. If we pass down to $L$, $\kappa$ is still present but it is no longer  measurable (although it will be strongly inaccessible); the construction of $L$ removed its 0-1-valued measure. ``Height'' is in this sense a looser metaphor than ``width''.\footnote{I thank Neil Barton for making this point.}

As \cite{sep-independence-large-cardinals} observes, even though we can construct, via metamathematical techniques, examples of theories of incomparable consistency strength, it is a surprising fact that the ``natural'' large cardinal axioms studied by set theorists appear to be linearly ordered (indeed, well-ordered) by consistency strength. Moreover, the research program known as \emph{large cardinals from determinacy}, associated with Martin, Steel, and Woodin \citep{koellner2010large}, explores the consequences of large cardinal hypotheses for descriptive set theory and analysis. An example is the case of the axiom of projective determinacy (PD). The projective sets are those $A \subseteq \R$ that can be produced via the finite iteration of certain operations on Polish spaces. PD says that for any such $A$, a certain two-player game on it is determined, i.e., one of the players has a winning strategy; for the purposes of our discussion, this may simply be taken as a generalization of desirable descriptive set-theoretic properties such as Lebesgue measurability, the property of Baire, and the perfect set property. Under $V = L$, PD is false and there are projective sets that are not Lebesgue measurable, etc.; under suitable large cardinal assumptions, however, PD is true. These considerations give rise to a case for Platonism about the existence of large cardinals and their consequences; when we enhance the consistency strength and interpretive power of our set theory in this mathematically natural way, we seem to discover truths ``lower down'' about the structure of $\mathcal{P}(\R)$. The full picture of the connection between large cardinals and determinacy goes deeper and is more sophisticated than I can present here; \cite{sep-large-cardinals-determinacy} gives a concise overview.\footnote{For the purposes of this paper, I will not formally dispute the claim that the program has settled the case for the acceptance of large cardinals --- I am primarily concerned with claims to have settled the width phenomena, not the height phenomena. For reasons discussed in sections \ref{maximizingpower} and \ref{hyperuniverse}, I am skeptical, but I think the jury is out.}

Significantly, for the most part, ``width'' and ``height'' questions are orthogonal. This is due to a family of results originating with the following theorem of Levy and Solovay: in a universe with a measurable cardinal $\kappa$, ``small'' forcing (i.e., forcing with a notion of size $< \kappa$) does not stop $\kappa$ from being measurable. Since such forcing is sufficient to alter the value of $2^{\aleph_0}$, it follows that CH is also independent from ZFC + ``a measurable cardinal exists.'' The result generalizes to other large cardinal notions, none of which are known to resolve CH. (However, there are ``width'' hypotheses that have large cardinal consistency strength, in particular two extensions of Martin's Axiom known as the Proper Forcing Axiom and Martin's Maximum. I will return to these hypotheses in section \ref{maximizingsets}.)

\subsection{Philosophical significance of the independence phenomena}
\label{philosophicalsignificance}
The pre-theoretic platonistic view about $V$ is that it is like $\mathbb{N}$, a categorical (i.e., unique) structure that satisfies the axioms of ZFC, but furthermore fixes a truth value for all sentences in the language of set theory. (I will discuss an attempt to ground this idea in section \ref{maximizingsets}.) \cite{steel2012godel} calls this view ``strong absolutism''.

I think it is important to note that this view is not directly challenged by any of the set-theoretic independence results; it is possible to maintain, in the face of them, realism in ontology about $V$ and realism in truth-value about its first-order theory. For example, suppose strong absolutism and then consider $L$. Then the fact that $L$ satisfies CH is irrelevant to the truth value of CH, which is fixed by the true $V$. If $V \not = L$, $V$ can see ``from the outside'' that $L$ is defective, in that it omits some sets that really exist --- in other words, there is at least one set-sized collection of sets that $L$ ``refuses'' to gather together into a set. Thus, $L$ does not force the strong absolutist into pluralism about the truth value of any sentences that vary between $V$ and $L$. These considerations apply equally to any other inner model construction.

What about outer models? If we are thoroughgoing platonists about $V$, there are no genuine sets outside of $V$ and therefore the idea of constructing a larger model of set theory is incoherent; specifically, $V$ does not have $V$-generic sets for any forcing notion $\mathcal{P}$, so the forcing construction does not get off the ground. In the context of relative consistency proofs, this issue can be metamathematically finessed by forcing against set models of finite fragments of ZFC (which can be proven to exist within ZFC itself, via the reflection results of Montague). So all the independence results discussed so far are intelligible without talk of actually expanding the universe.

Nevertheless, the idea of expanding $V$ itself by forcing is robust enough that set theorists do commonly speak of taking extensions of the universe. In particular, \cite{hamkins2011set} describes a ``naturalist account of forcing'' which gives a theoretical basis for taking this talk at face value. For any forcing notion $\mathcal{P}$, one can, within $V$, construct class models $\overline{V} \subseteq \overline{V}[G]$; there will be an elementary embedding of $V$ into $\overline{V}$, and $\overline{V}[G]$ will be a forcing extension of $\overline{V}$ by a $\overline{V}$-generic set $G \in V$. So outer models, like inner models, can be viewed as class models within $V$. But the strong absolutist can level exactly the same criticisms against them. The class models produced by forcing will not instantiate the correct levels of the true cumulative hierarchy; in some cases they can be seen ``from the outside'' (i.e., from the perspective of the true $V$) to be ill-founded. So again, the independence phenomena do not inherently push us into pluralism about the concept of set.

\subsection{The independence phenomena as part of mathematics}
\label{independenceismath}
At this point, I will endorse two views concerning the width phenomena. The first is the contention of \cite{magidor2012some} that it will not do to dismiss them as inherently metamathematical in character, irrelevant to the working mathematician --- indeed, the independence result for Whitehead groups came as a very surprising intrusion of higher set theory into a problem that was perceived as purely algebraic, and attempts after the fact to dismiss it as a ``merely'' set-theoretic problem are a kind of gerrymandering.

If one accepts that sentences like Whitehead's problem (whether there is a non-free Whitehead group) or Kaplansky's conjecture (whether there can be a discontinuous homomorphism between certain kinds of Banach algebras) remain properly mathematical questions, even after having been found independent of ZFC, then it follows that they pose a challenge not merely for set-theoretic foundational programs, but for alternative foundations as well: we can benchmark those alternative proposals by how they answer the independent questions. I will return to this issue in section \ref{alternatefoundations}.

\subsection{The ``dream solution'' to the independence phenomena}
\label{dreamsolution}
The other view I want to endorse is that of \cite{hamkins2011set} that at this point, we cannot hope to find an intuitively evident principle, analogous to the existing axioms and \emph{intrinsically} justified by the concept of set, which decides these questions. Given such a principle (Hamkins calls it the ``dream solution''), we are already so well acquainted with universes that violate it that we will not be convinced that it identifies something truly essential to the concept of set itself. We cannot recover platonism via the naive continuation of the axiomatic method.

Hamkins takes this further. On his view, the independence phenomena have demonstrated that there exist multiple, equally valid concepts of set. These concepts can be arranged to form a set-theoretic \emph{multiverse}, each world of which is a model of ZFC; Hamkins gives a formal description of this multiverse, one that entails an extensive anti-realism about concepts such as the ordinal hierarchy, countability, and well-foundedness. For him, the truth value of CH can vary across the set-theoretic multiverse, and this is the end of the matter: CH simpliciter has no truth value. I should emphasize that my endorsement \emph{ab initio} of Hamkins's attitude to the ``dream solution'' does not entail a similar endorsement of his multiverse view. I will return to the status of Hamkins's multiverse vis-a-vis more realist views in sections \ref{maximizingpower} and \ref{hyperuniverse}.

\section{Maximizing structures}
\label{maximizingstructures}
At this point, it may seem as though I have ruled out all the philosophically motivated avenues for resolving the continuum problem. However, instead of relying on the intuitive acceptability of axioms, we can appeal to the philosophical motivations for the set-theoretic program itself. Such a program could pick out philosophically \emph{better} models of set theory, and all of these models could agree on the truth value of CH --- this without rejecting Hamkins's claim that there are \emph{legitimate} models of both CH and its negation.

What goals does set theory serve as a foundation for mathematics? Structuralist critiques of set theory as a foundation often focus on the failure of set theory's ontology and proof system to describe the means by which mathematicians actually reason. These objections strike me as missing the point. The foundational goals served by ZFC are not primarily about enabling the straightforward translation of working mathematics into a formal system. Rather, the set-theoretic universe is ``Cantor's paradise''\footnote{Hilbert's phrase.}, in which seemingly disparate or incommensurate kinds of mathematical structure exist and can be studied together --- for example, the monster group, the complex numbers $\mathbb{C}$, and the $\omega_1$-Aronszajn tree. Thus, one might endorse an analogue of Shapiro's \citeyearpar{shapiro1997philosophy} ``coherence principle'' --- intuitively, ``all structures that can possibly exist, should exist'' --- and take as a Platonistic objective for set theory the principle of maximizing structures. The best set theory is then the one that realizes the most structures.

How can we formally cash out the idea of maximizing structures? \cite{maddyvlmaximize} provides one suggestion: maximize the \emph{isomorphism types} available. This principle can be used, for example, to argue against $V = L$ as an axiom, because it precludes the existence of the set $0^\#$ (a subset of $\N$ that codes a certain metamathematical property). It follows from this that if $V$ contains $0^\#$, no set in $L$ is isomorphic (from the point of view of $V$) to the level $V_{\omega+1}$ of $V$ at which it first appears --- thus, $V=L$ can be interpreted (after negotiating some distinctions, which I have elided, between syntax and semantics) as failing to maximize the availability of isomorphism types.

I think a significant problem with this suggestion (one that will generalize to other attempts at maximizing structures) is that there is no neutral standpoint from which to judge whether two objects are isomorphic (and therefore whether they represent one or two distinct isomorphism types); any such judgment must occur from the point of view of a particular model of set theory. For example, suppose we have conceptions of two competing models $V$ and $V'$ of set theory, but not a conception of how one is contained in the other; how are we to tell whether $a \in V$ and $b \in V'$ are isomorphic? But the situation is not improved when we consider the case where one model is contained in the other, because the condition of being isomorphic is not absolute under set forcing. Here is a straightforward example suggested to me by John Steel: the first-order theory $T$ of dense linear orderings without endpoints is $\omega$-categorical (it has exactly one isomorphism type of size $\aleph_0$, instantiated by the rational numbers $\Q$) but is not categorical in any uncountable cardinality. Let $M$ be some model of set theory, and let $A, B \in M$ be non-isomorphic models of $T$ of cardinality $\aleph_1$. Then, extend $M$ to $M[G]$ by forcing to collapse $\aleph_1$ so that it becomes a countable ordinal; $A$ and $B$ still satisfy the same first-order theory, but are now countable, so they have become isomorphic to each other and to $\Q$. (Interestingly, \cite{baldwin1993}, motivated explicitly by the idea that this merging of isomorphism types by forcing is a pathological phenomenon, give combined constraints on first-order theories and forcing notions that prevent this from happening.)

Can we rescue this characterization of structure-maximizing? I think there is a fundamental barrier: a natural conception of ``mathematical structure'', as it is used by working mathematicians and then interpreted within a set-theoretic ontology, will include non-absolute properties that depend on non-absolute relationships with $\N$ (such as countability) and $\R$. An example is the Suslin line, as discussed in section \ref{width}, which is characterized by two such non-absolute properties: having the countable chain condition and being non-isomorphic with $\R$. The existence of such a line $S$ is independent, because it follows from $\diamondsuit$ and is therefore true in ``narrow'' universes like $L$, but fails in a ``wide'' universe satisfying $\mathrm{MA}(\omega_1)$. But conversely, there is no non-free Whitehead group in $L$, but there is one under $\mathrm{MA}(\omega_1)$.

So if we accept this conception of mathematical structure, reducing the universe may \emph{add} structures as well as removing them, and enlarging it may remove structures as well as adding them. On this characterization of structure, can we maximize structures? I conjecture that this goal is impossible: some structures are unable to peacefully coexist in Cantor's Paradise and will force us to choose between them. Specifically:

\begin{conjecture}[Informal]
There exist sentences $\phi_1$ and $\phi_2$ in the language of set theory, each describing the existence of a mathematical structure, such that $\ZFC + \phi_1$ and $\ZFC + \phi_2$ are each consistent, but $\ZFC + \phi_1 + \phi_2$ is inconsistent.
\end{conjecture}

This conjecture is true when $\phi_1$ is the principle $\diamondsuit$ (read as ``a $\diamondsuit$-sequence exists''\footnote{Specifically, $\diamondsuit$ asserts the existence of a sequence of subsets $A_\alpha$ of $\aleph_1$, for $\alpha < \aleph_1$, such that for every $A \subseteq \aleph_1$, the set $\{\alpha \in \aleph_1 \mid A_\alpha = A \cap \alpha\}$ is stationary.}) and $\phi_2$ is ``a non-free Whitehead group exists'' --- but I think many working mathematicians might dispute the claim that a $\diamondsuit$-sequence is a bona fide structure, instead viewing it as a purely set-theoretic artifact. My (largely uninformed) speculation is that the conjecture is still true when $\phi_1$ is replaced by the sentence ``a Suslin line exists''.\footnote{On the other hand, it might be possible to start with a relativization of MA or PFA that preserves Suslin lines, as in \cite{todorcevic2011forcing}, and then still obtain a non-free Whitehead group.} But here is some more grounded speculation: the incompatible combinatorial phenomena in ``wide'' and ``narrow'' universes mean that we will be able to fill in \emph{some} natural $\phi_1$ and $\phi_2$. So I believe that the project of maximizing structures ends up being incoherent. I will return to the implications of this in section \ref{alternatefoundations}.


\section{Maximizing sets}
\label{maximizingsets}
To see how strong absolutism (that is, realism about $V$ as a definite totality) has fared in the face of the independence phenomena, it is instructive to look at Martin's [\citeyear{martin2012completeness}] exposition of the informal argument for the uniqueness of $V$. I note that Martin's actual position is subtle and does not literally endorse the argument as it is presented here --- I am presenting it not as part of a discussion of Martin's view, but because I think it illuminates the original, pre-independence-era motivation for set-theoretic platonism.

Suppose $V$ and $V'$ are models of set theory with the same ordinals; we will argue by an informal version of transfinite induction that they must be equal at every level of the cumulative hierarchy, and therefore equal overall. Certainly they must agree at level $0$: this is just to say that $V_0$ and $V_0'$ are both the empty set. Suppose now that they agree up to level $\alpha$ of the cumulative hierarchy, i.e., $V_\alpha = V_\alpha'$. Then, for any $x \in V_{\alpha+1}$, we can apply an informal version of the Axiom of Comprehension to collect a corresponding set $f(x) = \{y' \in V_\alpha' \mid y' \in x\} \in V_{\alpha+1}'$. Since $V'$ satisfies the Axiom of Extensionality, $f$ is an injection from $V_{\alpha + 1}$ to $V_{\alpha+1}'$; but since $V$ does as well, the corresponding map defined by $g(x') = \{y \in V_\alpha \mid y \in x'\}$ is also an injection and is the inverse of $f$. Therefore, $f$ is an isomorphism and we can use it to identify $V_{\alpha+1}$ with $V_{\alpha+1}'$, as before. The case where $\alpha$ is a limit ordinal is trivial, since any disagreement at a limit $\alpha$ must have been introduced at some level $\beta < \alpha$. $\square$

The significance of this argument is not in its ability to persuade the anti-realist or multiversist. Nonetheless, let's examine in detail the point at which the multiversist objects to it. Suppose that $V$ is a ``wide'' universe satisfying $\mathrm{MA}(\omega_1)$, and $V'$ is its $L$; our straw multiversist will maintain, for purposes of argument, that $V$ and $V'$ are equally good universes of set theory. Now, let  the two models will agree up to some level $\alpha$, where $\alpha > \omega$, and begin to disagree at $V_{\alpha + 1}$. Let $x \in V_{\alpha+1}$ be one of the sets that doesn't appear in $V_{\alpha+1}'$; when the absolutist tries to produce $f(x)$, the multiversist retorts that comprehension cannot be applied, because the property $y' \in x$ has no meaning within $V'$, where $x$ does not exist.

This objection is good as far as it goes. But the absolutist can reply: the failure of $V'$ to collect these elements into a set constitutes evidence that $V'$ is defective. After all, once the existence of $V$ and $x$ is conceded, there is nothing conceptually unclear about the property ``being a member of $x$''. Why, then, does $V'$ refuse to collect the elements of $V_\alpha'$ that satisfy it into a set? In essence, the universist is arguing that that $V$ is categorical because given two competing notions of set over a common set of objects, the more permissive one is superior --- given a set $X$ and a collection $Y$ of its elements, there are no grounds on which we can deny that $Y$ is a genuine subset of $X$. This gives us a basis for a genuinely Platonistic approach to set-theoretic truth: the true universe $V$ is the one that contains as many sets as possible, and the goal of set theory is to maximize sets.\footnote{The idea may originate with \cite{Godel1964-GODWIC}, who used it as part of an argument against $V = L$.}

Is there a mathematical program that can be viewed as maximizing sets? In fact, \cite{magidor2012some} proposes the forcing axioms --- MA, the Proper Forcing Axiom (PFA), and Martin's Maximum (MM) --- as formalizations of this intuition that the most permissive notion of set is the best. Intuitively, the forcing axioms identify a class of ``mild'' forcing notions, then assert that the results of applying those forcing notions are already available within the current universe. For example, MA applies to forcing with partial orders $\mathcal{P}$ that satisfy the countable chain condition (``c.c.c.''); among other desirable properties, these forcings preserve cardinals. $\mathrm{MA}(\omega_1)$ asserts that for any c.c.c. partial order $\mathcal{P}$ and any family of dense sets $D$ in $\mathcal{P}$ satisfying $|D| \leq \omega_1$, there is already a $D$-generic filter $F$ on $\mathcal{P}$. PFA and MM generalize this to larger classes of forcings, with MM giving the most general class for which a forcing axiom is consistent. Metaphorically, if we use these mild forcings to make our concept of set more and more expansive, the final result is a universe satisfying the relevant forcing axiom. Moreover, PFA and MM prove that $2^{\aleph_0} = \aleph_2$, so they decide CH in a natural way.\footnote{However, they do not decide GCH, or indeed fix the values of any beth number above $\beth_1$.}

The problem is that this seems to require viewing forcing in a realist sense as ``adding sets'' to the universe --- and if we accept this, then it's hard to know when to stop. If there is set-theoretic structure outside of our current universe and we can access it via forcing, why should we stop at mild forcing? Without this restriction, we can start from a universe satisfying MM, then force to restore CH (by collapsing $2^{\aleph_0}$ to equal $\aleph_1$) and then even an $L$-like principle such as $\diamondsuit$.

One could accept the forcing axioms concomitantly with the belief that non-mild forcings are pathological, because they destroy important features of the original universe; this would solve the immediate difficulty just presented. (Indeed, there is something intuitively pathological about collapsing a cardinal.) But the problem quickly reappears, since the forcing axioms are not ``stable'' with respect to their own classes of mild forcings. For example, starting from a universe that satisfies MM, one can use a forcing that is mild according to MM's own definition of mildness (in fact, a c.c.c. forcing) to obtain $2^{\aleph_0} = \aleph_3$, which will destroy MM.


Note that this is a disanalogy between outer and inner models, or between maximality and minimality, since $L$ is absolute under its own construction. With inner models, we can descend to the bottom, but with outer models there seems to be no top that we can climb to.

%To see whether proposed resolutions of the width phenomena are genuinely platonistic, it is instructive to look at Martin's [\citeyear{martin2012completeness}] exposition of the informal argument for the uniqueness of $V$. This argument makes clear the original, pre-independence motivation for set-theoretic platonism. Consider two allegedly distinct universes of set theory, $V_1$ and $V_2$, both satisfying our pre-theoretic intuitions about sets and sharing the same ordinals. Suppose that up to some level of the cumulative hierarchy, they are the same. Then consider any set $A_1$ appearing at the next level of the hierarchy of $V_1$. We can associate $A_1$ with a set $A_2$ at the next level of $V_2$ by the following procedure: collect, via an informal notion of comprehension, all elements of $V_2$ satisfying the \emph{property} of being contained in $A_1$ within the context of $V_1$. Thus, we associate every $A_1$ with an $A_2$, and this mapping is an isomorphism. The difficulty with this argument is of course that a global concept of set has been smuggled in, inside the notion of ``property.'' Nonetheless, the intuition is clear: the hierarchy of sets should be categorical because given two competing notions of ``set'' or ``collection'' over a common set of objects, the more permissive one is superior --- in other words, given a set $X$ and a collection $Y$ of its elements, there are no grounds on which we can deny that $Y$ is a genuine subset of $X$. This gives us a basis for a brand of platonism: the true universe $V$ is the one that contains as many sets as possible, and the goal of set-theoretic platonism should be to maximize sets.






%It is common to disparage $V=L$ as an axiom on the grounds that it is ``restrictive'' (cf. Foreman's survey) but it is significant that this ``restrictiveness'' is not literally a restrictiveness of either sets or structures. With regard to sets, we have the familiar problem that if V does equal L in a Platonistic sense, then the true universe is its own L and we have not excluded any sets by considering only L. With regard to structures, we see that just as V=L eliminates some objects like Whitehead groups, it also guarantees others like Suslin lines. Rather, the ``restrictiveness'' of V=L is in its incompatibility with large cardinal axioms, starting at the level of a measurable cardinal.

\section{Maximizing interpretive power}
\label{maximizingpower}
The \emph{inner model program} is probably the most significant contemporary attempt to complete set theory and resolve CH. Many notable people have contributed to it mathematically, but its most prominent advocates \emph{qua} philosophy are Steel and Woodin. In the discussion that follows, I will somewhat conflate their philosophical views, or perhaps rely on Steel as a philosophical interpreter of Woodin's technical program; the attendant dangers are evident but I think this is necessary in order to maintain focus.\footnote{Specifically, Woodin has historically advocated many different (and incompatible) technical programs with different philosophical justifications. The one discussed here is the one represented in his most recent work, which is also the closest to Steel's view. For a detailed history, see \cite{sep-continuum-hypothesis}.} The philosophical goal served by the inner model program is the maximization of interpretive power --- see in particular Steel \citeyearpar{feferman2000does,steel2012godel} --- and the technical goal is the construction of inner models that are compatible with the existence of very large cardinals (at the level of a supercompact and above). The key philosophical tenets of the program, invariant historically across several different technical directions, are realism about the existence of these large cardinals (about the ordinal hierarchy itself, and about the non-absolute properties of the ordinals asserted by large cardinal hypotheses), and about the consequences of their existence in descriptive set theory (for example, projective determinacy).

Two difficulties immediately present themselves. One is that, by the results discussed in section \ref{height}, CH appears to be entirely orthogonal to questions of interpretive power as expressed through the large cardinal hierarchy: for all known candidate theories $T$ that assert the existence of large cardinals, $T$, $T + \mathrm{CH}$, and $T + \neg \mathrm{CH}$ are all equiconsistent. The other is that to harness the interpretive power of large cardinals, a theory does not need to assert that large cardinals actually exist. As \cite{hamkins2011set} points out, the Shoenfield absoluteness theorem guarantees that for any reasonable\footnote{Specifically, any constructible theory. This includes all the recursively axiomatizable theories.} theory $T$, the existence of a countable transitive model of $T$ is absolute between $V$ and $L$, and thus between $V$ and any forcing extension of $V$. So if $T$ is something like ``ZFC plus the existence of arbitrarily large supercompact cardinals'', $T$ is incompatible with $V = L$, as are all large cardinals above a measurable. But ``ZFC, plus $V = L$, plus the existence of a countable transitive model of $T$'' is consistent if ``ZFC plus the existence of a strong inaccessible with arbitrarily large supercompacts below it'' is --- so the large cardinal realist is committed to the consistency of this theory as well.

Even before discussing specifics of the inner model program, it will be helpful to frame the discussion in terms of how it purports to respond to these two objections. To the second objection, Steel (ibid.) replies that due to the mathematical considerations described in section \ref{height}, specifically their natural well-ordering by consistency strength, large cardinals are unique among proposals to expand the interpretive power of ZFC in terms of their systematizing influence. Therefore, accepting the low-complexity consequences (such as arithmetical consistency sentences and the existence of countable transitive models) of large cardinals, while maintaining skepticism about large cardinals themselves, can be criticized as an \emph{instrumentalism}, comparable to instrumentalism about unobservables in physical science. In Steel's \citeyearpar{feferman2000does} phrase, it is philosophically unsatisfactory in the same sense as the assertion, ``There are no electrons, but mid-sized objects behave as if there were.'' So on this view, any ``first-class'' (in the sense of both interpretive power and philosophical faithfulness to the theory of large cardinals) model of set theory must contain \emph{all} the ordinals, and those ordinals must retain their requisite large cardinal properties, and therefore the model will satisfy PD, etc.

As for the first objection, the inner model program seeks to pick out a \emph{preferred} model of ZFC with large cardinals that will decide CH --- so its claims to resolve CH will rest on the philosophical justification for this preference. Woodin's ``Ultimate L'' program, more or less, is to obtain an $L$-like inner model that, like $L$, will support a detailed structural analysis, but unlike it will be compatible with the existence of large cardinals at the level of a supercompact and above. In one commonly discussed possible outcome, this model will satisfy $L$-like principles such as GCH and $\diamondsuit$.\footnote{As \cite{sep-continuum-hypothesis} points out, there are other ways the conjectures could turn out, including some possibilities where CH fails, but my critique applies in those eventualities as well.} The sense in which it maximizes interpretive power is via Steel's \emph{generic multiverse} proposal, as follows. Start from a model $V$ with (for example) arbitrarily large supercompact cardinals. Consider the universe of proper class models that are mutually accessible from $V$ via set forcing; since set-sized forcing cannot alter the properties of more than set-many cardinals, every model in the multiverse will still have arbitrarily large supercompacts. \cite{steel2012godel} then proves that the theory of this multiverse is expressible in the ZFC language of the original model. The truth value of CH will necessarily vary across the multiverse, but the multiverse may have a unique element definable in the multiverse language, the \emph{core}. Pending outcomes of Woodin's conjectures \citep{woodin2011suitable}, the core will be ultimate L. So we can accept the axiom ``V = Ultimate-L'' with confidence that no interpretive power has been lost, since any omitted structure is available --- in a ``first-class'' model with all the ordinals and arbitrarily large supercompacts --- via set forcing. Then we may take the fact that CH is true in Ultimate-L to mean that CH is true.

This worldview seems to imply that the only thing wrong with $V = L$ as an axiom was its incompatibility with large cardinals; the point is made explicitly by \cite{woodin2009continuum}. But what reason is there to think that $V$ \emph{should} support a particular kind of structural analysis --- even supposing that this analysis is the only known way\footnote{Since, as discussed previously, the forcing axioms do not settle GCH.} to answer open questions about its properties? If we seek analogies of this in other fields of mathematics, we find seemingly parallel phenomena; for example, in complexity theory, difficult questions are often studied under oracle relativizations \citep{fortnow1994role}, which can provide a simpler setting that nonetheless illuminates the original problem. But no one would say that answering the relativized question in itself answers the original question, or argue naturalistically that the original question has been superseded by the relativized question. So the strong absolutist who is a proponent of principles that are incompatible with ``V = Ultimate-L'' (for example, MM), can mount the following challenge: the inner model theory program is answering a relativized analogue of CH (specifically, relativized to the inner model Ultimate-L), but this is not an answer to the question itself. If the primary motivation for adopting ``V = Ultimate-L'' is simply that it answers our unanswered questions, from a Platonistic standpoint this appears to be an \emph{adjudication} or even a \emph{dismissal} of those questions, rather than a \emph{discovery} of their solution.

To make the point more explicit, imagine a proponent of MM as an axiom for set theory, motivated by the set-maximizing philosophical goals described in section \ref{maximizingsets}; call him Straw Magidor. If we ask Straw Magidor, ``why is there no Suslin line?'', he replies, ''the Suslin line was incompatible with the true concept of set.'' This is what I have in mind as a properly Platonistic  resolution of an independence phenomenon. It is not an instance of the ``dream solution'', since no one is claiming that MM is intuitively evident. Nor is it necessarily a claim that MM is intrinsically justified, in the sense of G\"odel and Koellner, on the basis of the concept of set. Straw Magidor may maintain instead that MM is not contained in the original concept of set, but rather in a refinement of that concept that deserves on Platonistic grounds to supersede the original.\footnote{In fact, I think this is a reasonable reading of the \emph{real} Magidor's position: he states explicitly that he is interested in extrinsic rather than intrinsic justifications, and the title of his paper (which doubles as the ``slogan'' of his program) is ``Some Set Theories Are More Equal.'' But I must disclaim the identification of my straw men with their real counterparts.} The point is that Straw Magidor can claim to have \emph{discovered}, in some sense, that there is no Suslin line. In contrast, if we ask Straw Woodin ``why is there no Whitehead group?'', Straw Woodin replies, ``it was incompatible with a fine-structural analysis of the universe --- but since no interpretive power has been sacrificed, you can still study it by forcing $\mathrm{MA}(\omega_1)$ over the core model.'' And if we ask Straw Steel, ``why is CH true?'', he replies, ``once we translate the continuum problem into the multiverse language, it becomes inexpressible unless we reframe it as a question about the core model, in which case it's true.'' But why, then, should we accept the reframing? These responses, by comparison with Straw Magidor's, seem to betray an essential anti-realism about the truth values of sentences that can vary across the worlds of the generic multiverse.\footnote{Returning briefly to the possibility that the preferred inner model will violate CH, or even have a Whitehead group: substitute an independent question that is true under a forcing axiom, but not under any candidate inner model axiom, and the argument proceeds \emph{mutatis mutandis}. As \cite{sep-continuum-hypothesis} describes, there may be a Platonistic means of deciding between candidate ``completionist'' inner model axioms, via the structure theory of rank-into-rank embeddings --- but the Platonistic case for choosing one at all is still lacking.}

Could we then analyze set-theoretic truth simply as ``truth in every world of the generic multiverse''? At first glance, this would fulfill some of the program's goals by recognizing the existence of large cardinals and projective determinacy as global truths. However, \cite{woodin2009continuum} actually argues against this characterization, based on the idea that the set of $\Pi_2$ generic-multiverse truths would be ``too simple'', in the sense of being captured by set-sized models. Without engaging directly with this argument, there is another problem: there will seemingly be some ``accidental'' generic-multiverse truths, due to limitations of the construction. In particular, the generic multiverse is not closed under class forcing (this would break its ``amalgamation'' property, which asserts that any two of its worlds can be made equal via forcing extensions). Suppose the core of the generic multiverse satisfies GCH. Then, since set forcing can only change the value of the continuum function in set-many places, no world of the generic multiverse will violate GCH in class-many places, e.g., by having $2^{\aleph_\alpha} = \aleph_{\alpha + 2}$ for all regular $\aleph_\alpha$, even though we know that this condition is equiconsistent with ZFC via Easton's class forcing technique. So, if we want to study a universe satisfying this condition in the generic multiverse setting, we have to study it via set models; we cut the core model off at a suitable large cardinal $\kappa$ and force over $V_\kappa$. But now we have abandoned our insistence on ``first-class'' models of set theory, and opened the door back up to Hamkins's anti-realism about the ordinal hierarchy, the large cardinals, and PD. In this way, the generic multiverse view may ultimately undermine its own anti-instrumentalist motivations. I will say more about the status of the anti-instrumentalist argument in the next section.


%These programs are an attempt to to recover the advantages of $L$ --- in particular, a fine-structural analysis that resolves the ``width'' problems --- while avoiding the incompatibility of $L$ itself with the large cardinals, and thereby retaining the descriptive set-theoretic consequences of large cardinals (such as projective determinacy). But in doing so, they eliminate certain structures from the universe, in particular, structures like Whitehead groups that are available under the forcing axioms. The natural reply is that since the core model still retains maximum interpretive power, we can still study these structures, either by taking forcing extensions of the universe, or within suitably well-behaved (e.g., countable transitive) set models of ZFC. But in either case, the structures will still be second-class citizens relative to those that actually exist in the preferred model --- we will have decided for the Suslin line and against the Whitehead group, in the service of the seemingly unrelated goal of restricting the universe to one that is easier to study. Magidor's argument that these phenomena should not be lightly dismissed by working mathematicians applies equally to set theorists.

%Moreover, if we allow phenomena like Whitehead groups to be studied via set models (and this is actually necessary in the case of certain constructions that cannot be performed within the generic multiverse, such as class forcing), we open the door again to a particular kind of antirealism about large cardinals. As \cite{hamkins2011set} observes, the existence of countable transitive models of a given set of axioms is absolute between $V$ and $L$. This supports a perspective in which $V=L$ holds and large cardinals do not actually exist, but their consistency is witnessed by countable transitive set models. Steel \citep{feferman2000does} considers this position (as with others in which large cardinals are rejected, but their consistency and other arithmetic consequences accepted) to be an unsatisfactory kind of instrumentalism --- it is as though we said, ``electrons do not exist, but the physical world behaves as if they do.'' But if set models are a suitable venue in which to confine phenomena like Whitehead groups, then they are also a suitable venue to confine phenomena like large cardinals --- and just as the consistency of the existence of Whitehead groups doesn't compel belief in their existence, the consistency of large cardinals should no longer compel belief in their desirable descriptive set-theoretic consequences, such as projective determinacy.

%This criticism is generally applicable to programs like Woodin's axiom (*) which are argued for on the basis that they have some regularizing effect on the universe under forcing, such as generic absoluteness. Either there are no forcing extensions of the universe, in which case the justification is vacuous, or there are, in which case we are explicitly acknowledging that sets and structures exist outside of the model satisfying the new axiom.)

%This problem is even more pressing in the case of phenomena that are commonly studied via class forcing, under which the generic multiverse is not closed. For example, suppose that the ultimate L conjecture is proven and ultimate L becomes the preferred venue in which to do mathematics. Then ultimate L, which satisfies GCH, will be the core of the generic multiverse and no world in the multiverse will violate GCH at class-many ordinals. In this sense, the interpretive power of the generic multiverse necessarily falls short, and there are mathematical phenomena that would have to be studied in set models.

\section{Other possibilities}
\subsection{The hyperuniverse program}
\label{hyperuniverse}
The \emph{hyperuniverse program} \citep{arrigoni2013hyperuniverse} rests on a third kind of multiverse picture, different from those of Hamkins and Steel. The hyperuniverse $\mathcal{H}$ is defined, relative to a particular model $V$ of set theory, to be the set of all countable transitive models of ZFC that exist in that universe. It is consistent with ZFC that the hyperuniverse is empty, so in order to have a theory of interest we need to augment ZFC with additional consistency strength. Large cardinals are effective for this, but a different candidate is suggested by the program itself: the \emph{Inner Model Hypothesis} (IMH). This axiom asserts, speaking loosely and eliding some difficulties in formalization, that any sentence $\phi$ achievable by forcing over $V$ is already realized in some inner model of $V$. This axiom has large cardinal consistency strength and guarantees that $\mathcal{H}$ contains models with large cardinals, but it rules out the existence of any large cardinals (at the level of an inaccessible or higher) in $V$ itself. So it is a putative counterexample to the key premise of the anti-instrumentalist argument described in the previous section, i.e., the claim that asserting the actual existence of large cardinals is the only mathematically fruitful direction for maximizing interpretive power. To reuse Steel's analogy, the IMH is like an empirically adequate scientific theory with no electrons.

The hyperuniverse program seeks to formulate principles that guarantee good properties for $\mathcal{H}$, or for subsets of $\mathcal{H}$ that contain \emph{preferred} models (this preferment may be for technical or philosophical reasons). Then it proposes to investigate the consequences of those principles in $V$ itself --- including, potentially, CH or its negation. I will not discuss the status of these goals in detail because at the present time of writing, the program does not have a fully mature strategy for resolving CH. (A strengthening of the IMH, the \emph{Strong Inner Model Hypothesis}, implies that CH is false and that the continuum must be quite large\footnote{Specifically, SIMH implies that $2^{\aleph_0} > \aleph_\alpha$ for any $\alpha$ countable in $L$.}; however, its status is still tentative because it has not yet been proven consistent relative to established large cardinal hypotheses.)

Nevertheless, I think that the program, as it stands, already offers an interesting interpretation of the independence results that have been discussed. We can maintain strong absolutism about $V$ and regard $\mathcal{H}$, or some preferred subset of it, as the space of \emph{epistemically} possible universes of set theory. Most of them will not be \emph{metaphysically} possible on this view, since they will satisfy sentences that are not true in $V$ and be thereby incompatible with the unique true concept of set, as instantiated by $V$. But nonetheless, the default outcome of an independence phenomenon is the creation of new epistemically possible worlds --- which may subsequently be dispelled by the discovery of new properties of $V$, or new restrictions on $\mathcal{H}$.

Meanwhile, from the point of view of multiversism, I think the hyperuniverse is our best current formalization of what the multiverse might look like. As discussed in section \ref{maximizingpower}, the Steel-Woodin generic multiverse seems too restrictive, in that it can realize at most one of the regular cardinal exponentiation functions $2^{\aleph_\alpha} = \aleph_{\alpha + 1}$ and $2^{\aleph_\alpha} = \aleph_{\alpha + 2}$. On the other hand, the \cite{hamkins2011set} multiverse seems far too permissive, in particular in its anti-realism about the concepts of well-foundedness and $\mathbb{N}$. Specifically, it satisfies a principle called \emph{well-foundedness mirage}: every universe $V$ is ill-founded from the point of view of another universe $W$. On this view, every universe contains at least one set $\epsilon$ that it believes to be an ordinal, but such that there is no fact of the matter about whether $\epsilon$ is well-founded. Moreover, Hamkins thinks $\epsilon$ can be as low as $\omega$, i.e., he rejects the concepts of the standard model $\N$ of the natural numbers and the true theory of first-order arithmetic. I agree with \cite{barton2016multiversism} that this rejection also undermines the metamathematical posits needed to develop and discuss the multiverse in the first place, in particular the concepts of well-formed formula and proof. Even if we could stave off this collapse by asserting that $\epsilon$ must always be greater than $\omega$, i.e., that every universe must be an $\omega$-model, I find the idea that the concept of well-foundedness is \emph{never} secure untenable. By contrast, the hyperuniverse offers us a much more sober picture; since the worlds of $\mathcal{H}$ are transitive set models, they straightforwardly share global concepts of $\omega$, $\in$, and well-foundedness with each other and with $V$.

\subsection{Non-set-theoretic foundations}
\label{alternatefoundations}
Homotopy type theory (HOTT) is a novel foundational program that unites ideas from category theory and algebraic topology. Awodey, one of its creators, proposes it \citeyearpar{awodey2014structuralism} as a realization of philosophical structuralism about mathematics, in particular as a foundation that takes structure rather than set-theoretic ontology to be fundamental. But HOTT has many potential advantages on a technical level as well: a greater fidelity to mathematical practice, an easier pathway to computer-checkable and and computer-assisted proofs, and more dialogue between foundational efforts and ordinary working mathematics.

I take the upshot of the discussion in section \ref{maximizingstructures} to be that the concept of structure, as it is commonly used by working mathematicians, appears to be set-theoretically relative: in certain extreme (but nonetheless probative) cases, our only approach to the question of whether a structure (e.g., the Whitehead group) actually exists is to think about it in terms of axioms that extend ZFC. So on one level, the independence phenomena challenge the possibility of an independent conception of structure: how will a structuralist foundation decide whether the Whitehead group is real? If the only way to do so is by importing analogues of set-theoretic principles such as $\diamondsuit$ or MA, this undermines the claim that the new foundational scheme is truly independent of set theory.

%Just as the width phenomena challenge set-theoretic platonism, they argue against structuralist views in which something other than set theory is to provide the foundation for mathematics. Without set theory, how are we to understand the phenomenon of structures --- say, a group and a linear order --- which exist only under incompatible assumptions? The role of set theory in informing us about which structures actually exist is reinforced, not undermined, by the independence phenomena.

On the other hand, the phenomenon of ZFC-independent statements in ordinary mathematics means that an alternative foundational system could potentially reveal truths about set theory. For example, Kaplansky's conjecture states that every homomorphism $h: A \to B$, where $A$ is the Banach algebra $C_0(X)$ for $X$ a Hausdorff space and $B$ is an arbitrary Banach algebra, must be continuous. If CH is true, then the conjecture is false, i.e., there exist spaces with a discontinuous homomorphism that provide a counterexample. If MA is true, however, then the conjecture is true. If, as I claimed in section \ref{independenceismath}, the Kaplansky conjecture is a properly mathematical question and not a pseudoproblem or artifact of the choice of set-theoretic foundations, a proof in HOTT that it is true (of the first-class mathematical objects posited by HOTT) would be evidence that CH is actually false, or more conservatively that we should prefer set theories in which it is false.

\section{Conclusions}
Where do we go from here? First of all, none of the considerations discussed here rule out the possibility of a new program (perhaps the hyperuniverse program), or a modification of one of the programs already mentioned, that would resolve CH on the basis of a recognizably Platonistic goal. But it's also possible that CH could be resolved on purely naturalistic grounds: set-theoretic practitioners and working mathematicians could together come to a de facto agreement to extend ZFC with axioms that decide CH.

A comparison with the Axiom of Choice (AC) is instructive. Historically, AC was very controversial \citep{sep-axiom-choice}, but it now enjoys near-universal acceptance by working mathematicians along with the rest of ZFC as a foundation for mathematical practice. This is surely not because a consensus emerged among mathematicians that AC was in fact intrinsically justified by the concept of set! Rather, it seems that G\"odel's construction of $L$ put to rest the most significant concern about AC, that it might be inconsistent, after which the path was clear for mathematicians to view it as an essential and harmless convenience. As \cite{hrbacek1999introduction} put it: ``the irreplaceable role of the Axiom of Choice is to simplify general topological and algebraic considerations which otherwise would be bogged down in irrelevant set-theoretic detail.'' Similarly, the forcing axioms could become useful to functional analysts\footnote{I am somewhat hazy on the details of this scenario. My understanding is that several nice theorems in functional analysis have undesired counterexamples under $V = L$ or CH, which are then ruled out by MA, PFA, or the Open Coloring Axiom. Examples include the aforementioned Kaplansky conjecture on Banach algebras, the theorem that all countably tight Hausdorff spaces are sequential \citep{balogh1988countable}, and the theorem that all automorphisms of the Calkin algebra are inner \citep{farah2011all}.}, which could further their methodological acceptance among working mathematicians more generally. Alternately, anti-realism about set theory among working mathematicians could foster acceptance of Ultimate-L as the preferred venue for mathematical practice --- to people who are inclined to see ``purely set-theoretic'' questions like CH as pseudoproblems, the perception that ``V = Ultimate-L'' dismisses those questions could be a feature instead of a bug.

Working mathematicians often seem to intuit that the bulk of their subject matter does not actually depend on set-theoretic considerations; for them, working in ZFC has more the character of a notational choice than an ontological commitment. I am therefore sympathetic to programs like Feferman's \citeyearpar{feferman1992little} that seek to ground this intuition both technically and philosophically: by developing as much mathematics as possible in weaker systems than ZFC, then justifying a realist attitude towards those systems and the entities they posit. Furthermore, due to philosophical considerations not discussed here, I am personally sympathetic to Feferman's anti-realism about transfinite mathematics, and to explorations like that in \cite{rathjen2014indefiniteness} of formal systems that attempt to capture this attitude. But at the same time, I think the project of maximizing consistency strength and interpretive power via the large cardinal hierarchy is extraordinarily philosophically compelling. And I also agree with the suggestion of \cite{cummings2012challenges} that whatever else the independence phenomena are, they are also the subject matter of combinatorial set theory, a significant branch of mathematics in its own right and one that should not be suppressed as an inadvertent byproduct of a foundational program that completes ZFC. Inasmuch as these sympathies point to any coherent view about mathematical truth, it is a tiered one --- realism about arithmetic and some transfinite mathematics, a guarded realism about ZFC and countable transitive models containing large cardinals, a guarded skepticism about large cardinals, and an attitude on which the truth values of sentences like ``a Suslin line exists'', or CH itself, might indeed turn out to vary modally.

\cite{steel2012godel} is correct that we should not allow a fragmentation of mathematical practice into incompatible domains --- in his phrase, we want ``all our flowers to bloom in the same garden.'' But I think philosophical pluralism does not have to endanger the unity of practice that we presently enjoy. I am optimistic that different foundational programs, with contradictory philosophical objectives, can thrive together without erecting fences in the garden of mathematical practice --- or seriously challenging the identification of that garden with Cantor's Paradise, an identification which has borne much fruit and continues to do so.

\section{Acknowledgments}
This work is tremendously indebted to conversations with two people in particular --- John Steel and Clare Stinchcombe --- without whose mathematical help and philosophical insights it would not have been possible. Remaining misconceptions are of course my own. I would also like to thank the other participants, besides Clare, in the informal EFI reading group at UC Berkeley in spring 2013, in particular Alex Kruckman and Noah Schweber. Dimitris Tsementzis and Douglas Blue made helpful comments on a draft. Finally, I would like to thank the organizers and the other participants in the SOTFOM II conference, in particular the anonymous reviewers of my extended abstract.

\bibliography{philosophy}

\end{document}
