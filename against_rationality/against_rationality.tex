\documentclass[letterpaper,12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}

\setlength{\voffset}{-1 in}
\addtolength{\textheight}{2.15 in}

\renewcommand{\implies}{\rightarrow}
\newcommand{\pr}{\text{pr}}
\newcommand{\Prob}{\text{Pr}}
%\newcommand{\PR}{\text{PR}}
\newcommand{\dom}{\mathrm{dom\ }}
\newcommand{\rng}{\mathrm{rng\ }}

\newcommand{\A}{\mathfrak{A}}
\newcommand{\N}{\mathbb{N}}

\renewcommand{\O}{\mathfrak{O}}
\newcommand{\T}{\mathfrak{T}}
\newcommand{\proves}{\vdash}

\renewcommand{\phi}{\varphi}

\newtheorem{tractability}{Criterion}[section]
\newtheorem{guidance}{Criterion}[section]

\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}

\begin{document}
\title{Against the Possibility of a Formal Account of Rationality}
\author{Shivaram Lingamneni}
\maketitle


\begin{abstract}
I analyze a recent exchange between Adam Elga and Julian Jonker concerning unsharp (or imprecise) credences and decision-making over them. Elga holds that unsharp credences are necessarily irrational; I agree with Jonker's reply that they can be rational as long as the agent switches to a nonlinear valuation. Through the lens of computational complexity theory, I then argue that even though nonlinear valuations can be rational, they come in general at the price of computational intractability, and that this problematizes their use in defining rationality. I conclude that the meaning of ``rationality'' may be philosophically vague.
\end{abstract}

\section{Introduction}
Part of decision theory, at least in the philosophical sense, appears to be providing a formal account of rationality --- telling the hypothetical ``rational agent'' what to do. But it seems that such an account must avoid two traps. For one, it must separate ``rationality'' (achievable, ideally susceptible of rule-based description) from ``insight'' (difficult, presumably not so susceptible). It seems reasonable to expect that rationality requires the avoidance of Dutch Books, but not that it requires independently reinventing the theory of special relativity. Rationality seems to be defined as only a limited fragment of our cognitive potential; it does not coincide with our most general notions of ``human reason.''

Another potential trap is a ``no-theory'' account of rationality: ``just do the right thing.'' Certainly, yielding the best available action (at least within the context of some constrained space of problems) seems to be a necessary condition for any theory of rationality. However, simply stipulating that the agent take the best action seems at best a deeply unsatisfying account. For one, it fails to describe how the agent should go about achieving rationality. But furthermore, even in finite/discrete situations where brute force is applicable, it seems to leave the actual content of rationality unexplained.

Call these Pitfall 1 and Pitfall 2. Following a suggestion by Scott Aaronson that computational complexity theory has significant things to tell us about philosophy --- in particular, about issues of human cognition and logical omniscience --- I wish to reread a recent exchange between Adam Elga and Julian Jonker in complexity-theoretic terms.

\section{Background}
I will give an approximate reconstruction of the Elga-Jonker exchange; I must warn the reader that the reconstruction is neither complete nor fully accurate. In the case of Elga, I omit much of the argument, since my concerns differ somewhat from his. In the case of Jonker, I must emphasize that I am reconstructing only an intermediate version of his view --- one of his stepping stones, as it were --- that I agree with much more than I agree with his final conclusion.

Elga argues as follows:

\begin{enumerate}
\item
Consider the following situation (hereafter ``Jellyfish Bag''). Imagine that an insane man in the street pulls the following objects out of a bag: a red toothbrush, a live jellyfish, and a green toothbrush. What should your degree of belief be that the next object he will remove will be a toothbrush? There seems to be little or no relevant evidence with which to fix a credence.
\item
For this and for a wide class of propositions, it is (apparently) impossible to assign a precise degree of belief ("sharp credence").
\item
Consider now the following situation (hereafter ``Good Book''). Given an unknown event X, you are offered two bets, A and B. Bet A costs \$10 and wins you \$25 if X is true. Bet B costs \$10 and wins you \$25 if X is false.
\item
A rational agent must buy at least one of the bets. A sharp agent maximizing expected utility will buy A if he has $P(X) \geq .4$, and B if he has $P(X) \leq .6$ (both if $P(X) \in [.4, .6]$). But any agent who buys both bets will make a sure profit of \$5, whether X comes true or not. (Elga calls this situation a "good book", but I will reverse the direction and say that the agent has been offered a Dutch Book, in the classical sense of the term.)
\item
If an agent's credence in X is uncertain --- in particular if it may lie outside the interval $[.4, .6]$, the agent does not appear to be constrained to buy at least one of the bets. (Elga evaluates and rejects a number of candidate principles that could so constrain the agent.)
\item
Elga concludes that in fact, it is irrational to have unsharp credences, since the unsharp agent may fail to recognize and exploit Dutch Books.
\end{enumerate}

Elga does not resolve the apparent contradiction between the Jellyfish Bag and the Good Book; his conclusion appears to be that the rational agent must have a sharp, precise credence for the event of a toothbrush. Jonker replies (keeping in mind my previous caveats):

\begin{enumerate}
\item
Implicit throughout Elga's discussion is the assumption that bets be evaluated in a ``value-additive'' or ``linear'' way; if you value bet A at $v(A)$ and bet B at $v(B)$, linearity dictates that you value A and B together at $v(A) + v(B)$.
\item
In practice, many situations seem to call for nonlinear valuations. In particular, a risk-averse agent can be sublinear. Faced with a small bet, he may value it positively and accept it. But faced with a million copies of the same bet, he may reject them negatively and reject them, because he is afraid to go bankrupt.
\item
Similarly, the utility of physical objects can be superlinear. For example, a car with an empty gas tank and a jerrycan of gas are worth more together than apart.
\item
A natural perspective on the unsharp agent in the ``Good Book'' scenario is that her valuations may be superlinear. Due to her uncertain credence in the event X, she may not consider either bet A or bet B worthwhile in isolation. But in that case, she should recognize that together they form a Dutch Book and buy them both.
\item
Model unsharp credences by credence intervals, e.g., $[.3, .7]$. The following rule accommodates both the sharp agent, who wishes to maximize expected value, and the unsharp agent with interval-valued preferences, who wishes to buy the Dutch Book: choose the action which maximizes (over all possible actions) the minimum expected utility (over all credence values in the interval).
\end{enumerate}

Jonker's ultimate rejection of MMEU is motivated by the simplistic nature of the interval-valued credence model, also by the way it seems to conflate unsharpness with risk aversion. Certainly, MMEU (as a maximin principle) does not seem to describe all possible rational responses to unsharpness. An optimistic agent with ``nothing to lose'' (perhaps identifiable with the risk-seeking agent) might prefer a maximax principle, choosing the bets with the greatest possibility of gain, no matter how unlikely that possibility is. In between these two, we have the possibility of collapsing the unsharp agent into a sharp, risk-neutral agent, whose first-order credences are the expected values of her second-order credence distribution. I do not wish to argue that the maximin response to unsharpness is the only response, or a universally applicable one. My discussion will depend only on these contentions:

\begin{enumerate}
\item
MMEU is a successful counterexample to Elga's claim that no decision principle constrains the unsharp agent to buy at least one bet from the Good Book. The maximization in MMEU is over four possibilities: buy neither, buy A, buy B, or buy both. The minimum expected utility from buying neither is \$0, and the minimum expected utility from buying both is \$5, so it is impossible for the agent to buy neither.
\item
In at least \emph{some} situations, MMEU is in fact the ideally rational response to unsharp beliefs. This is because it successfully models an attitude I will call ``absolute risk aversion'': the unwillingness to countenance any possible loss.
\end{enumerate}

The first of these is, I think, evident. The second is perhaps more controversial, but the rationality of risk aversion has been extensively discussed by philosophers and economists. I will offer just one scenario in which absolute risk aversion seems justified: bankruptcy. Imagine that an agent is offered a package of bets, the expected value of which is high, but which admits at least one possible outcome in which she loses all her money. Furthermore, she is confident that in the future, she will be offered a Good Book --- as long as he has the money to buy it in the first place. Alternately, imagine that the loss outcomes from the bets are very high, and that in the agent's society, the penalty for unpaid debt is to be sold into a lifetime of indentured servitude.

This suggests a computational rereading of Elga's argument. Elga has shown that for an unsharp agent, there exist Dutch Books such that every individual bet of them is not worthwhile in isolation. Thus, rationality requires the unsharp agent to have the ability to recognize Dutch Books; otherwise she risks missing out on sure gain. However, linear (or value-additive) decision principles such as expected utility maximization (hereafter EUM) are not powerful enough to recognize Dutch Books. The unsharp agent must replace EUM with something like MMEU.

I am a frequentist, so the most basic objections to the MMEU picture that occur to me are methodological. Why should the unsharp agent be able to quantify the exact nature of her unsharpness? Saying that imprecise beliefs are described by precise intervals of credence (or precise second-order belief distributions) seems to be introducing a false precision --- and once we have abandoned the Ramseyan argument that precise credences can be elicited by measuring the agent's propensity to bet, it is unclear how we can measure these higher-order beliefs, no matter how we represent them. These objections lead rapidly into abstract concerns and touch on a longstanding controversy in the philosophy of probability, and I will not discuss them further here.

A second basic objection, and one mentioned by Jonker, is that MMEU does not seem like the last word in decision principles. Expected utility maximization neatly avoided Pitfall 1; it gave a simple criterion for action. Now that we have accepted this more elaborate principle, is the door open for us to require more and more complex criteria for rationality? This is the objection I intend to pursue formally here. In order to do it, I will introduce some basic notions from computational complexity theory.

\section{Computational complexity and philosophy}
Unlike recursion (or ``computability'') theory, in which the main objects of study are problems that cannot be solved by any computer, computational complexity theory studies the relative hardnesses of problems that computers can solve. Speaking very loosely, the problems we are ordinarily accustomed to solving with computers (arithmetical operations, sorting, shortest paths in maps, etc.), are in the complexity class P, meaning that they can be solved within a time that is polynomial in the size of the input.

There is a natural class of prima facie harder problems, known as NP. Intuitively, problems in NP have the following form: they can be computed by an algorithm that ``guesses'' a solution from an exponential search space, then verifies it in polynomial time. The canonical problem of this type is SAT, or Boolean satisfiability: the question of whether a formula of propositional logic is true under some assignment of truth values to the atoms. Checking whether a particular assignment satisfies the formula is easy (i.e., polynomial-time), but given $n$ atoms, there are $2^n$ possible assignments overall --- thus, the brute-force solution to SAT requires time at least exponential in the size of the input. P is clearly contained in NP. Although it is strongly suspected that in fact P $\not =$ NP, this has not been proven; it is considered one of the major unsolved problems in contemporary mathematics. 

(Introduce Karp and Cook reducibility; introduce NP-completeness and NP-hardness.)

The \emph{exponential time hypothesis} of Impagliazzo and Paturi is slightly stronger than P $\not = NP$. It states that SAT and 3SAT (and some related NP-complete problems) cannot be solved faster than time proportional to $2^n$ --- metaphorically, that for these problems, it is not possible to improve on brute-force search. As with P $\not =$ NP, the ETH is unproven but widely believed. Recent work by Williams bolsters its plausibility by relating it to other widely believed conjectures.

I will idiosyncratically refer to the problem of deciding whether a propositional formula is a tautology as VAL (for ``validity''). The specific form of VAL where the formulae are 3-ary conjunctions of positive or negated atoms (by analogy with 3SAT) will be called 3VAL. Under the exponential time hypothesis, VAL and 3VAL require at least exponential time (since they are straightforwardly Cook-equivalent to SAT and 3SAT, respectively).

\subsection{The tractability criterion}
Complexity theory gives a natural (partial) formalization of Pitfall 1. Rational agents have limits on their computational power; they cannot be expected to perform arbitrarily difficult optimizations. So it seems natural to propose the following \emph{tractability criterion}:

\begin{tractability}
Assume $\text{P} \not = \text{NP}$. Any decision principle proposed as a constraint on rational agents must have a polynomial-time algorithm.
\end{tractability}

I hold this condition to be necessary, but not sufficient --- but I will postpone discussion of the converse principle until my conclusion.

How much you believe the tractability criterion will depend on how much you believe logical omniscience is a problem for Bayesianism. Personally, I believe that it is a very serious problem, and that it has not been adequately addressed. But a Bayesian might contend that computationally intractable decision principles still usefully describe \emph{idealized} Bayesian decision-making --- even if this ideal is radically unattainable. Against such a position, I offer a different counterargument, one related to the second pitfall. 

\subsection{The guidance criterion}
I hold that the exponential time hypothesis almost precisely captures the intent behind Pitfall 2. If it is true, then there are search problems for which no help is possible; every successful algorithm is (modulo constant-factor speedups) a brute-force search in disguise. So this motivates my proposed \emph{guidance criterion}:

\begin{guidance}
Assume the exponential time hypothesis. A decision principle that requires the agent to solve NP-hard problems is \emph{content-deficient}, because it requires the agent to try every alternative and pick the best one --- it offers no \emph{guidance}.
\end{guidance}

\section{Technical results}
A disclaimer: every result in this section is more or less trivial. The least trivial result, hardness of DUTCHBOOK, is proved in greater generality by Paris in the context of verifying the consistency of probabilistic beliefs. I include proofs so that the technical material can be compared at every step with the underlying intuitions that motivate the hypotheses, in hopes of justifying that the results are founded on essential aspects of the problems, rather than accidents of the mathematical formalism.

The natural generalization of Elga's Good Book scenario is the problem of betting on books of propositional formulae. In the case we are interested in, these propositional formulae consist of Boolean combinations of atomic events, for some set of $n$ atoms. Furthermore, we will require that the agent regard all $2^n$ possible combinations of these atomic events as logically possible --- equivalently, that the agent is not aware of any logical implications among the events. If such a thing seems outlandish, consider the atoms $\{f_i \mid 1 \leq i \leq 31\}$, where $f_i$ denotes the event that I will eat falafel for lunch on the $i$th day of January. The $f_i$ may not probabilistically independent, but they certainly seem to be logically independent. There really are $2^{31} = 2147483648$ distinct possibilities for my lunches.

%For the sake of simplicity, throughout the technical discussion, we will take a ``bet'' on a formula $\phi$ to be a promise to pay \$1 if $\phi$ comes true. The price of a bet on $\phi$ will then correspond to the agent's personal probability for $\phi$. Later, when discussing risk aversion, we'll consider more expensive bets, but all the proofs should translate seamlessly to the new setting.

The concept of Dutch Book is also in need of some formal clarification. Ian Hacking points out that in cases where the bookie knows more facts than the bettor, the result may be a trivial Dutch Book. For example, when the bettor pays \$0.5 for a \$1 bet that the quarter will land tails, but unbeknownst to him the bookie has provided a two-headed quarter, the bettor has been Dutch Booked in the sense that he loses money under every possible outcome. I think this is not properly in the spirit of the original definition of Dutch Book. The relevant quantification is over all outcomes the bettor perceives as logically possible --- and this includes the excluded possibility of tails. It may also include events to which the agent assigns probability zero --- for example, if we model the heights of men with a real-valued normal distribution, we consider it logically possible for a man to be exactly six feet tall, even though we assign this event a probability of 0. For this reason, I will define a Dutch Book as one that pays off over every outcome in the agent's state space, whether or not the agent assigns the outcome nonzero probability.

Definition. A decision principle for betting books is a recursive function that takes in a book of propositional bets $B$ and a representation $C$ of the agent's credences, then outputs what bets the agent should buy. (EUM and MMEU are decision principles in this sense.)

Definition. Let DUTCHBOOK be the following decision problem. Given a book of propositional bets over $n$ atoms, does there exist a package of bets that yield a profit under all $2^n$ outcomes?

Claim: DUTCHBOOK is Karp-reducible to MMEU over the class of all propositional books. (In other words, MMEU is harder than DUTCHBOOK, or ``contains'' it.)

Proof. Given a book of bets we intend to test for Dutchness, we construct an agent whose degree of belief in every atom is uncertain between $0$ and $1$, inclusive. We apply MMEU to this agent; the agent will buy a package of bets if and only if they are a Dutch book. (Intuitively, the agent who is completely unsure of every proposition can only justify betting when victory is assured, no matter the outcome.)

If we wish to avoid applying MMEU to agents with extremal beliefs (personal probabilities of $0$ and $1$), we can simply substitute beliefs that are sufficiently close to $0$ and $1$. I omit a detailed proof, but it should suffice to replace $0$ with an $o$ satisfying
$$0 < o < \frac{1}{\sum_i w_i} (\min_{i\not = j} |w_i - w_j|)$$
where the $w_i$ are the payoffs of the various bets. Replace $1$ with $1 - o$; both of these are clearly computable in polynomial time. $\square$

Claim: DUTCHBOOK is Karp-reducible to any principle that computes an optimal decision for a completely risk-averse agent.

Proof. As above. The risk-averse agent can only buy Dutch books. (As we mentioned previously, the fact that the risk-averse agent assigns probability 0 to an outcome does not exclude it from the definition of Dutch Book.) $\square$

Claim. DUTCHBOOK over the class of books consisting of bets on atoms or negated atoms is in P.

Proof. The book consists entirely of prices on bets on $A$ and $\neg A$, for various logically independent atomic propositions A. Sort the bets by which proposition they describe. If the book gives two distinct prices for some A, there is a Dutch Book (sell the bet at the higher price and buy at the lower); if it prices $\neg A$ at something other than 1 - Price($A$), there is a Dutch Book (either buy both bets or sell both bets). If neither of these is true for any $A$, the bookie's beliefs are Kolmogorov consistent and there is no Dutch Book against him. $\square$

Elga's original example falls into this class of books. But if we generalize to arbitrary propositions, the problem becomes harder --- the definition of Dutch Book quantifies over all $2^n$ possible outcomes, and we can harness the difficulty of the problem to perform difficult computations.

Claim. DUTCHBOOK over the class of books consisting of arbitrary propositions is NP-hard.

Proof. Fix a propositional formula $\phi$. Apply DUTCHBOOK to the book consisting of a single bet on $\phi$, priced at \$0.5. This book is Dutch if and only if $\phi$ is a propositional validity (or tautology); this is a Karp reduction of VAL to DUTCHBOOK. $\square$

This seems somewhat cheap. Naturally, if we confront our agent with an arbitrarily complex propositional formula, we might expect bewilderment. In particular, it is not clear that the expected-utility-maximizer can do better on this problem. But consider what happens when we replace VAL with 3VAL.

Definition: Call a book of bets on propositions of the form  $(A \land B \land \neg C)$, i.e., conjunctions of three atoms or negated atoms, a 3-Book.

Claim: DUTCHBOOK over the class of 3-Books is NP-hard. Furthermore, under the Exponential Time Hypothesis, it requires at least exponential time.

Proof. Fix an instance of 3VAL, i.e., a formula $\phi$ that is the disjunction of $n$ clauses of the abovementioned form. Construct the following book: for each clause, offer a bet, priced at $\$1$, that pays $\$(n+1)$ if the clause comes true.

This book is Dutch if and only if $\phi$ is a validity. If $\phi$ is a validity, then under every possible outcome, $\phi$ must be true, so at least one of its disjuncts must be true, so buying every bet costs $\$n$ and pays at least $\$n+1$, for a sure gain of at least $\$1$. Conversely, if $\phi$ is not a validity, then under the propositional valuation that makes it false, no bet pays off and every package loses money.

It follows that the construction is a Karp reduction of 3VAL to DUTCHBOOK. But 3VAL is Cook-equivalent to 3SAT, which is NP-complete. $\square$

Now we can see the computational advantage of sharpness:

Claim: The sharp Bayesian agent with an oracle for personal probabilities can evaluate 3-Books in polynomial time. Performing this evaluation requires only polynomially many beliefs.

Proof. The agent examines every bet in isolation, computes its expected payoff against her personal probability, and accepts the bet iff the payoff is positive. This is linear time, or $O(n)$. The number of 3-ary propositions she can be asked to bet on is bounded above by $\binom{2n}{3}$, which is $O(n^3)$. $\square$

\section{The dilemma}
Assume either that it can be rational to have unsharp credences, or that it can be rational to be maximally risk-averse. Assume P $\not =$ NP and the Exponential Time Hypothesis. Let DEC be a proposed decision principle for the evaluation of bets, such as expected-utility-maximization or MMEU. I argue that DEC must be caught on one horn or the other of the following dilemma:

\begin{enumerate}
\item
DEC runs in polynomial time.
\begin{enumerate}
\item
In general, DEC is not powerful enough to identify Dutch Books.
\item
Specifically, consider the following class of decision problems: 3-Books with $n$ clauses, viewed by either by a maximally risk-averse agent or an unsharp agent whose uncertainty about each atom is $[p, 1-p]$ for $p < \frac{1}{n+1}$. There must exist instances of this class where DEC does not yield the optimal action (buying the book if and only if the clauses form a validity).
\item
A generalization of Elga's argument applies to DEC: DEC is an inadequate condition on rationality, because it does not constrain unsharp and risk-averse agents to take a rational action (exploiting a Dutch Book).
\end{enumerate}
\item
DEC is powerful enough to identify Dutch Books.
\begin{enumerate}
\item
DEC must (in general) run in exponential time.
\item
DEC has fallen victim to Pitfall 1: it is computationally intractable and therefore represents an unachievable standard of rationality.
\item
DEC has fallen victim to Pitfall 2: since it instructs the agent to perform an exponential-time brute-force search, it is content-deficient.
\end{enumerate}
\end{enumerate}

EUM is gored by the first horn, and MMEU by the second --- and there is no happy medium between them.

Note that in order to generalize (``scale'', perhaps) the problem to larger books, we required the existence of agents with increasingly wide unsharp belief intervals: if the agent's interval does not go below $\frac{1}{n+1}$, then she can justify buying the bets in the 3-Book based on their expected utility alone. This is a problem, but not, I think, a serious one. For one, these thresholds are shrinking only reciprocally, while the time complexity of the 3-Books is increasing exponentially; it's easy to imagine someone whose dubiety extends from 1\% to 99\%, but $2^{100}$ is already astronomically large.

What is the proper interpretation of this dilemma? Someone convinced of the necessity of sharp credences (Elga's position) might read it as another reductio ad absurdum for unsharp ones. Someone convinced of the rationality of unsharp credences might take it as a reductio for Elga's argument. Someone else might attack the role of the combinatorial formalism in the argument. Personally, I endorse both unsharp credences and the original form of Elga's argument; I will instead deny the universal applicability of the tractability and guidance criteria, and moreover of any \emph{particular} principle of rationality. But I will not deny the relevance of computational concerns to rationality; I think their relevance extends well beyond the betting situations discussed so far.

%There's a technical conjecture here that could be relevant. Fix a hard instance of SAT. Can we assign events to the propositions (e.g., Alan Hajek's ``George W. Bush turns into a prairie dog'') such that the agent will plausibly assign a very low probability to each disjunct? Yes --- as long as the instance isn't also a validity!

%Another question of potential interest is whether the expected-utility-maximizer can have a polynomial-time algorithm that gives a consistent assignment of probability to arbitrary propositions. Such an algorithm would necessarily have to assign probability 0 to some logically possible propositions (otherwise, it would solve SAT), but it might still have the ability to capture interesting belief states. Conversely, a result showing that such an algorithm cannot exist (or must be degenerate) might have interesting implications for Bayesian decision theory in its own right.

%ANTICIPATE THE VAGUENESS ARGUMENT.

\section{Two more attacks}
\subsection{Combinatorial optimization as a fact of life}
NP-hard problems have a way of intruding into situations where one might not expect them. Robert (292) shows that they can arise naturally in Bayesian statistics, when estimating hyperparameters. But they abound in real-life situations as well. For example, the well-known Travelling Salesman Problem models the situation faced by a person who wishes to plan a trip that will visit $n$ cities exactly once, finally returning to his starting place. Finding the minimal such tour turns out to be NP-complete.

There is a slippery slope here. Let's say the the agent is an actual traveling salesman, and real money is riding on how fast he can cover his route. Does rationality require him to solve the TSP? It seems fairly clear that the answer is ``no'', because picking a suboptimal tour and getting back to work is preferable to waiting indefinitely for the optimal solution. In practice, he can try a fast-in-practice heuristic technique such as branch-and-bound; if it doesn't finish and further delay would lose him business, he can switch to a polynomial-time approximation algorithm like Lin-Kernighan or Christofides. But does rationality entail knowing these algorithms as well?

Some natural NP-hard problems (for example, set cover, which plausibly abstracts a situation that arises in personnel hiring) are hard to approximate accurately at all.

Herbert Simon's idea of the satisficing agent is an attempt to account for the cost of decision-making. But to the best of my knowledge, the theory remains at the level of psychology; it has not been made precise.

\subsection{Search problems and creativity}
The car-and-gas problem, suggested by Jonker, is an example of real-life nonlinearity. Let's say you want to go to the beach. A car with an empty gas tank and a jerrycan of gas, taken separately, are unhelpful. But combine them (appropriately) and all is well.

I think this scenario is like Elga's Good Book: it only looks tractable because it is so small. Reconstructing the reasoning:
\begin{enumerate}
\item
A rational agent should realize that neither the empty car nor the gas can get to the beach in isolation.
\item
A rational agent should realize that the car and the gas can be combined and driven to the beach; equivalently, an agent who does not realize this has violated rationality.
\end{enumerate}

What happens if we generalize this problem to $n$ objects? It seems that now we require the agent to be aware of $2^n$ different possible relationships among the proffered objects. Is the rational agent necessarily someone of limitless ingenuity, seeing all the possible uses for the objects around him?

I wish to emphasize that this version does not have a natural formalization as an NP-hard problem; specifically, it is difficult to imagine a presentation of the problem that does not give all weights for subsets of the items up front, at which point picking an optimal member is trivially $O(n)$.\footnote{A related problem, of picking the best allocation of items to groups, is maximal matching for hypergraphs, which I believe is NP-complete.}

But metaphorically, I think this example hints at Scott Aaronson's idea about the relationship between P and NP.

\section{Rationality --- a vague notion?}
At this point, I would like to undermine the tractability criterion as originally stated --- that it is necessary that any rationality principle have a polynomial-time algorithm. For although I have argued against the adoption of decision principles that can solve general instances of DUTCHBOOK, I have not argued against Elga's claim that a rational agent must solve his specific \emph{instance} of DUTCHBOOK, consisting of the two simple bets A and B.
\begin{quote}
I can only ask you to vividly imagine a case in which an agent rejects both bets A and B. Keep in mind that this agent cares only about money (her utility scale is linear), that she is certain in advance what bets will be offered, and that she is informed in advance that her state of opinion on the bet proposition will remain absolutely unchanged throughout the process. I invite you to agree with me that this agent has exhibited a departure from perfect rationality.
\end{quote}
And indeed I think Elga is right: a notion of rationality that doesn't require buying at least one of A and B is surely too weak. But what is the notion of rationality that Elga is appealing to? Can it be clarified?

As we have seen, the general problem here is intractable. But that does not seem to undermine Elga's invitation, even if we pass from his easy instance of 1-VAL to an easy instance of 3-VAL. What seems to matter is the real-world tractability of the problem --- the fact that we can work through the four cases in our heads --- not the existence of a general solution in polynomial time.\footnote{The formal analogue of this is perhaps the following: any specific instance of any problem is always considered solvable in $O(1)$. Once the input has been fixed, any procedure that outputs the right answer can be regarded as having run in constant time.} It would, in my opinion, be entirely reasonable to say that an unsharp rational agent is obliged to run MMEU on small problems --- perhaps up to systems with 16 distinct states --- and I think Elga's argument is persuasive as long as the problem can be solved ``fast-in-practice''.

So P may not be a necessary condition for real-world tractability. But is it sufficient? The consensus seems to be that it is neither. An exponential-time algorithm may be fast in practice if the input sizes are small, but a polynomial-time algorithm may be unusable if the exponent (e.g., $O(n^{1000})$) or the constant factor is too high. I think that to date, the most thorough investigation of the problem of modeling tractability is the computer-scientific one, and that its results have been largely negative and inconclusive.

My view is that ``rational agent'' is a notion analogous to the common law's elusive ``reasonable man'' --- when we invoke ``rationality'' we are appealing to an imprecise intuition rather than to something with ontological reality. It seems that our notion of rationality does (and should) vary with our standards of experience, education, and computational power.

\section{Acknowledgements}
Thanks to Julian Jonker, Wes Holliday, Roy Frostig, Justin Vlasits, and Sherri Roush for helpful discussions.
\end{document}
